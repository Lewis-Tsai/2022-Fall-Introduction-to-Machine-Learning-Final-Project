{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drcZacjAs_GF",
        "outputId": "201efab0-03bb-4734-f913-c3e89c86ee7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (23.1.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (0.29.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.51.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (4.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (14.0.6)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (3.19.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (21.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.11.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-gpu) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.2.2)\n",
            "2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ],
      "source": [
        "! pip3 install tensorflow-gpu\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "! pip3 install tqdm\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kzNGWq3zuPs"
      },
      "source": [
        "# **Parameter**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMWZqy8YmmEs",
        "outputId": "38f0f002-43fd-4907-b99b-0272abeac331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTcqMEfUCkm_"
      },
      "outputs": [],
      "source": [
        "# Original Data Info.\n",
        "n_pitch = 53\n",
        "n_feature = 43\n",
        "\n",
        "# Model Parameter\n",
        "input_size = n_feature\n",
        "target_size = n_pitch + 4 # +4 (addition) is oneset + start correction + end correction + velocity\n",
        "\n",
        "# Training Parameter\n",
        "BATCH_SIZE = 32          # Batch Sizes\n",
        "VALIDATION_RATIO = 0.1  # Validation Ratio to Input Data\n",
        "learning_rate = 0.001   # Learning Rate\n",
        "n_hidden = 2            # Hidden Units number\n",
        "\n",
        "# Data Parameter\n",
        "PIECE_LEN = 200          # (auto set by data) Extra +1 for START timestamp\n",
        "is_onset_index = 53\n",
        "start_correction_index = 54\n",
        "end_correction_index = 55\n",
        "velocity_index = 56\n",
        "\n",
        "ROOT = r'/content/drive/MyDrive/ML'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8iDHeXjo2Zc",
        "outputId": "7278593c-1c5c-41c3-f46c-0a20de7188fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ML\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/drive/MyDrive/ML')\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcLw90VlRmfe"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "# Delete incomplete unzipped folder and run this \n",
        "if 'irealpro_dataset_v2' not in os.listdir():\n",
        "    print('Creating Folder \\'irealpro_dataset_v2\\'')\n",
        "    os.mkdir('../irealpro_dataset_v2')\n",
        "    print('Extract to Folder \\'irealpro_dataset_v2\\'')\n",
        "    with ZipFile(r\"/content/drive/MyDrive/ML/irealpro_dataset_v2.zip\", 'r') as zObject:\n",
        "        zObject.extractall(path=r\"/content/drive/MyDrive/ML/irealpro_dataset_v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmoVUOkVMRUc"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJU0zll_pAsL"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3Tj17ylBnes",
        "outputId": "24cac4ad-ead9-4ab6-c16a-bc56311943ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "449\n",
            "449\n",
            "449\n",
            "449\n",
            "(449, 200, 43)\n",
            "(449, 1, 57)\n",
            "(449, 200, 57)\n",
            "Data Padded: 49\n",
            "Data with Non Start Decoder Input: 400\n",
            "Data PIECE_LEN: 200\n"
          ]
        }
      ],
      "source": [
        "dec_init_input = np.zeros((1, target_size))\n",
        "dec_init_input[0, start_correction_index] = 1.\n",
        "\n",
        "count_pad = 0 \n",
        "count_spilled = 0\n",
        "max_data = 50\n",
        "def load_npy_data(x_path, y_path, offset):\n",
        "    global count_pad, count_spilled, dec_init_input\n",
        "    x = np.load(x_path)\n",
        "    y = np.load(y_path)\n",
        "\n",
        "    if offset == 0:\n",
        "        Z_ = dec_init_input\n",
        "    else:\n",
        "        Z_ = y[offset-1].astype(np.float32)\n",
        "        Z_ = np.expand_dims(Z_, axis=0)\n",
        "        count_spilled += 1\n",
        "\n",
        "    if x.shape[0] >= offset+PIECE_LEN:\n",
        "        X_ = x[offset:offset+PIECE_LEN].astype(np.float32)\n",
        "        Y_ = y[offset:offset+PIECE_LEN].astype(np.float32)\n",
        "    else:\n",
        "        pad_count = offset + PIECE_LEN - x.shape[0]\n",
        "        X_ = np.pad(x[offset:], ((0, pad_count), (0, 0)), 'constant', constant_values=-1).astype(np.float32)\n",
        "        Y_ = np.pad(y[offset:], ((0, pad_count), (0, 0)), 'constant', constant_values=-1).astype(np.float32)\n",
        "        count_pad += 1\n",
        "    try:\n",
        "        assert X_.shape == (PIECE_LEN, input_size)\n",
        "        assert Z_.shape == (1, target_size)\n",
        "        assert Y_.shape == (PIECE_LEN, target_size)\n",
        "    except:\n",
        "        print('You got',X_.shape, Z_.shape, Y_.shape)\n",
        "        raise ValueError\n",
        "\n",
        "    return X_, Z_, Y_\n",
        "\n",
        "def generate_dataset(input_dir: str):\n",
        "    global max_data\n",
        "    # using tf.data.Dataset API to create dataset\n",
        "    x_paths = [] # input path\n",
        "    y_paths = [] # ans file path\n",
        "    offsets = [] # starting point of a piece\n",
        "    data_cnt = 0\n",
        "    for file_name in os.listdir(input_dir): # 'Scan Files'\n",
        "        if data_cnt==max_data-1: break\n",
        "        if file_name.endswith(\".ans.npy\"):\n",
        "            data_cnt+=1\n",
        "            y_path = str(os.path.join(input_dir, file_name))\n",
        "            x_path = str(os.path.join(input_dir, file_name[:-8] + \".npy\"))\n",
        "            assert os.path.exists(x_path), f\"corresponding input file {x_path} doesn't exist\"\n",
        "\n",
        "            # split and pad data into PIECE_LEN\n",
        "            y_content = np.load(y_path)\n",
        "            for offset in range(0, y_content.shape[0], PIECE_LEN):\n",
        "                y_paths.append(y_path)\n",
        "                x_paths.append(x_path)\n",
        "                offsets.append(offset)\n",
        "    dataset = []\n",
        "    for i in zip(x_paths, y_paths, offsets): # 'Read Files'\n",
        "        dataset.append(load_npy_data(*i))\n",
        "    print(len(dataset))\n",
        "    dataset = list(zip(*dataset))\n",
        "    # print(len(dataset[0]))\n",
        "    # print(len(dataset[1]))\n",
        "    # print(len(dataset[2]))\n",
        "    encoder_input_data = np.array(dataset[0])\n",
        "    decoder_input_data = np.array(dataset[1])\n",
        "    decoder_target_data = np.array(dataset[2])\n",
        "    print(encoder_input_data.shape)\n",
        "    print(decoder_input_data.shape)\n",
        "    print(decoder_target_data.shape)\n",
        "    perm_id = np.random.shuffle(np.arange(encoder_input_data.shape[0]))\n",
        "    encoder_input_data = encoder_input_data[perm_id]\n",
        "    decoder_input_data = decoder_input_data[perm_id]\n",
        "    decoder_target_data = decoder_target_data[perm_id]\n",
        "\n",
        "    return encoder_input_data, decoder_input_data, decoder_target_data\n",
        "\n",
        "encoder_input_data, decoder_input_data, decoder_target_data = generate_dataset(ROOT+\"/irealpro_dataset_v2\")\n",
        "\n",
        "\n",
        "\n",
        "print('Data Padded:', count_pad)\n",
        "print('Data with Non Start Decoder Input:', count_spilled)\n",
        "print('Data PIECE_LEN:', PIECE_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rw5ZzmBeZZ6"
      },
      "outputs": [],
      "source": [
        "# np.save(ROOT + '/encoder_input_data.npy', encoder_input_data)\n",
        "# np.save(ROOT + '/decoder_input_data.npy', decoder_input_data)\n",
        "# np.save(ROOT + '/decoder_target_data.npy', decoder_target_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_input_data = np.load(ROOT + '/encoder_input_data.npy', allow_pickle=True)\n",
        "# decoder_input_data = np.load(ROOT + '/decoder_input_data.npy', allow_pickle=True)\n",
        "# decoder_target_data = np.load(ROOT + '/decoder_target_data.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "2sDGfGtXsSEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysk_4cPxhPJw"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN1ooFJHhGvt"
      },
      "source": [
        "## *Define Loss, Accuracy, and Optimizer*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqzuy9Z9rTRj"
      },
      "outputs": [],
      "source": [
        "# Define Loss, Accuracy, and Optimizer\n",
        "def masked_loss_function(y_true, y_pred):\n",
        "    mask = tf.math.not_equal(tf.reduce_sum(y_true[:,:,:n_pitch], axis=2), -1*n_pitch)  # false if it is a padding time step\n",
        "    pitch_loss = tf.losses.categorical_crossentropy(y_true[:,:,:n_pitch], y_pred[:,:,:n_pitch])\n",
        "    onset_loss = tf.losses.binary_crossentropy(y_true[:,:,n_pitch:n_pitch+1], y_pred[:,:,n_pitch:n_pitch+1])\n",
        "    start_loss = tf.square(y_true[:,:,n_pitch+1] - y_pred[:,:,n_pitch+1])\n",
        "    end_loss = tf.square(y_true[:,:,n_pitch+2] - y_pred[:,:,n_pitch+2])\n",
        "    velocity_loss = tf.square(y_true[:,:,n_pitch+3] - y_pred[:,:,n_pitch+3])\n",
        "    total_loss = tf.reduce_sum([pitch_loss, onset_loss, start_loss, end_loss, velocity_loss], axis=0)\n",
        "    total_loss *= tf.cast(mask, total_loss.dtype)\n",
        "    return tf.reduce_mean(total_loss)\n",
        "\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    mask = tf.math.not_equal(tf.reduce_sum(y_true[:,:,:n_pitch], axis=2), -1*n_pitch)  # false if it is a padding time step\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "    pitch_acc = tf.reduce_mean(mask * tf.metrics.categorical_accuracy(y_true[:,:,:n_pitch], y_pred[:,:,:n_pitch]))\n",
        "    onset_acc = tf.reduce_mean(mask * tf.metrics.binary_accuracy(y_true[:,:,n_pitch:n_pitch+1], y_pred[:,:,n_pitch:n_pitch+1]))\n",
        "    start_loss = tf.reduce_mean(mask * tf.square(y_true[:,:,n_pitch+1] - y_pred[:,:,n_pitch+1]))\n",
        "    end_loss = tf.reduce_mean(mask * tf.square(y_true[:,:,n_pitch+2] - y_pred[:,:,n_pitch+2]))\n",
        "    velocity_loss = tf.reduce_mean(mask * tf.square(y_true[:,:,n_pitch+3] - y_pred[:,:,n_pitch+3]))\n",
        "\n",
        "    return pitch_acc, onset_acc, start_loss, end_loss, velocity_loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEEFfGMtuSrk"
      },
      "outputs": [],
      "source": [
        "checkpoint_filepath = './checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgTnwXkVDrq9"
      },
      "source": [
        "## Reinjection Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAltopwMDqwY",
        "outputId": "8ffbcde6-60d7-473b-c4aa-1fc8a2f0d8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [01:16<00:00,  2.61it/s]\n"
          ]
        }
      ],
      "source": [
        "from keras import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Concatenate, Dense, LSTM, Embedding, GRU, Attention, Lambda, Permute, Flatten\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(PIECE_LEN, input_size))\n",
        "encoder = LSTM(n_hidden, return_sequences=True, return_state=True)\n",
        "\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "states = [state_h, state_c]\n",
        "# Attention\n",
        "attention = Attention(n_hidden)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(1, target_size))\n",
        "decoder_lstm = LSTM(n_hidden, return_sequences=True, return_state=True)\n",
        "decoder_dense = Dense(target_size, activation='softmax')\n",
        "\n",
        "all_outputs = []\n",
        "all_attention = []\n",
        "inputs = decoder_inputs\n",
        "for _ in tqdm(range(PIECE_LEN)):\n",
        "    # Attention Update\n",
        "    # print(states.shape) \n",
        "    # print(encoder_outputs.shape)\n",
        "    context_vector, attention_weights = attention([tf.expand_dims([state_h, state_c],1), encoder_outputs], return_attention_scores=True)\n",
        "    all_attention.append(attention_weights)\n",
        "    # context_vector = Permute((2, 1))(context_vector)\n",
        "    # context_vector = Flatten()(context_vector)\n",
        "    # print(inputs.shape)\n",
        "    # print(context_vector.shape)\n",
        "    context_vector_h, context_vector_c = Lambda(lambda x: x[:,0,:,:], output_shape=(1,) + context_vector.shape[2:])(context_vector)\n",
        "    # print(context_vector_h.shape)\n",
        "    # print(context_vector_c.shape)\n",
        "    state_h = Concatenate(axis=-1)([state_h,context_vector_h])\n",
        "    state_c = Concatenate(axis=-1)([state_c,context_vector_c])\n",
        "    # Decoder Reinject\n",
        "    outputs, state_h, state_c = decoder_lstm(inputs, initial_state=states)\n",
        "    # Output\n",
        "    outputs = decoder_dense(outputs)\n",
        "    all_outputs.append(outputs)\n",
        "    # Update Decoder Input & State\n",
        "    inputs = outputs\n",
        "    states = [state_h, state_c]\n",
        "\n",
        "# Concatenate all predictions\n",
        "decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "\n",
        "# Define and compile model as previously\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# masked_loss_function , masked_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkqEk-ZYHZu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a06d91-0c4d-4464-c0bf-4051fd77e48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 449, 200, 43)\n",
            "(1, 449, 1, 57)\n",
            "(1, 449, 200, 57)\n",
            "Epoch 1/10\n"
          ]
        }
      ],
      "source": [
        "# Train model as previously\n",
        "print(encoder_input_data.shape)\n",
        "print(decoder_input_data.shape)\n",
        "print(decoder_target_data.shape)\n",
        "\n",
        "inputz = tf.data.Dataset.zip(tuple([tf.data.Dataset.from_tensor_slices((encoder_input_data)), tf.data.Dataset.from_tensor_slices((decoder_input_data))]))\n",
        "outputz = tf.data.Dataset.from_tensor_slices((decoder_target_data))\n",
        "dataset = tf.data.Dataset.zip((inputz, outputz))\n",
        "\n",
        "model.fit(dataset,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=10,\n",
        "          verbose=1,  \n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utf7XJS2phmu"
      },
      "source": [
        "### Predict Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HB1hWafv_5o"
      },
      "outputs": [],
      "source": [
        "def predict(test_file):\n",
        "    encoder_test_data = ...test_file...\n",
        "    decoder_test_data = np.zeros((BATCH_SIZE, 1, target_size))\n",
        "    return model.predict([encoder_test_data, decoder_test_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoS5QDH90qlQ"
      },
      "source": [
        "# **Midi Format Output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVMm4rWb19cN"
      },
      "outputs": [],
      "source": [
        "PATH = \"test_input\"\n",
        "test_file = np.load(\"preprocessed_dataset/irealpro_midi/Autumn Leaves_o0.mid.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtE2bX6fybvB"
      },
      "outputs": [],
      "source": [
        "import midi_np_translation.output2midi as output2midi\n",
        "\n",
        "test_result = predict(test_file)\n",
        "output2midi.output_to_midi(bass_ndarr=test_result.reshape(-1,52), ref_midi_path=\"input_midi/irealpro_transposed/Autumn Leaves_o0.mid\", output_path=\"yo_al.mid\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
