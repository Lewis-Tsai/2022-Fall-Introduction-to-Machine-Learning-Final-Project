{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Masking, Bidirectional, LSTM, TimeDistributed, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "VALIDATION_RATIO = 0.1\n",
    "\n",
    "PIECE_LEN = 128\n",
    "n_feature = 42\n",
    "n_hidden = 200\n",
    "n_pitch = 52\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npy_data(x_path, y_path, offset):\n",
    "    x = np.load(x_path)\n",
    "    y = np.load(y_path)\n",
    "    if x.shape[0] >= offset+PIECE_LEN:\n",
    "        return x[offset:offset+PIECE_LEN].astype(np.float64), y[offset:offset+PIECE_LEN].astype(np.float64)\n",
    "    else:\n",
    "        pad_count = offset + PIECE_LEN - x.shape[0]\n",
    "        x = np.pad(x[offset:], ((0, pad_count), (0, 0)), 'constant', constant_values=-1).astype(np.float64)\n",
    "        y = np.pad(y[offset:], ((0, pad_count), (0, 0)), 'constant', constant_values=-1).astype(np.float64)\n",
    "        return x, y\n",
    "\n",
    "def generate_dataset(input_dir: str):\n",
    "    # using tf.data.Dataset API to create dataset\n",
    "    x_paths = [] # input path\n",
    "    y_paths = [] # ans file path\n",
    "    offsets = [] # starting point of a piece\n",
    "    for file_name in sorted(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".ans.npy\"):\n",
    "            y_path = str(os.path.join(input_dir, file_name))\n",
    "            x_path = str(os.path.join(input_dir, file_name.removesuffix(\".ans.npy\") + \".npy\"))\n",
    "            assert os.path.exists(x_path), f\"corresponding input file {x_path} doesn't exist\"\n",
    "\n",
    "            # split and pad data into PIECE_LEN\n",
    "            y_content = np.load(y_path)\n",
    "            for offset in range(0, y_content.shape[0], PIECE_LEN):\n",
    "                y_paths.append(y_path)\n",
    "                x_paths.append(x_path)\n",
    "                offsets.append(offset)\n",
    "\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_paths, y_paths, offsets)).shuffle(100000)\n",
    "    train_dataset = train_dataset.map(lambda x_path, y_path, offset: tf.numpy_function(load_npy_data, [x_path, y_path, offset], [tf.float64, tf.float64]))\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(-1).cache()\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "train_dataset = generate_dataset(\"test_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 128, 42)]         0         \n",
      "                                                                 \n",
      " masking_19 (Masking)        (None, 128, 42)           0         \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirecti  (None, 128, 400)         388800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 128, 52)          20852     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 409,652\n",
      "Trainable params: 409,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input = tf.keras.Input(shape=(PIECE_LEN, n_feature))  \n",
    "x = Masking(mask_value=-1, input_shape=(PIECE_LEN, n_feature))(input) # Ignore Padded Data\n",
    "x = Bidirectional(LSTM(units=n_hidden, input_shape=(1, n_feature), return_sequences=True))(x)\n",
    "x = TimeDistributed(Dense(n_pitch, activation=\"softmax\"))(x)\n",
    "model = tf.keras.Model(inputs=input, outputs=x)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss/accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss_function(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(tf.reduce_sum(y_true, axis=2), -1*n_pitch)  # false if it is a padding time step\n",
    "    loss = tf.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    mask = tf.cast(mask, loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    mask = tf.math.not_equal(tf.reduce_sum(y_true, axis=2), -1*n_pitch)  # false if it is a padding time step\n",
    "    acc = tf.metrics.categorical_accuracy(y_true, y_pred)\n",
    "    mask = tf.cast(mask, acc.dtype)\n",
    "    acc *= mask\n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x)\n",
    "        \n",
    "        loss = masked_loss_function(y, pred)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = masked_accuracy(y, pred)\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.2939 Acc 0.8665\n",
      "Time taken for 1 epoch 3.017658233642578 sec\n",
      "\n",
      "Epoch 2 Loss 0.2861 Acc 0.8689\n",
      "Time taken for 1 epoch 3.299320697784424 sec\n",
      "\n",
      "Epoch 3 Loss 0.2786 Acc 0.8709\n",
      "Time taken for 1 epoch 2.4503231048583984 sec\n",
      "\n",
      "Epoch 4 Loss 0.2710 Acc 0.8735\n",
      "Time taken for 1 epoch 3.1006157398223877 sec\n",
      "\n",
      "Epoch 5 Loss 0.2638 Acc 0.8756\n",
      "Time taken for 1 epoch 2.3538990020751953 sec\n",
      "\n",
      "Epoch 6 Loss 0.2570 Acc 0.8785\n",
      "Time taken for 1 epoch 2.1065120697021484 sec\n",
      "\n",
      "Epoch 7 Loss 0.2502 Acc 0.8811\n",
      "Time taken for 1 epoch 2.371340036392212 sec\n",
      "\n",
      "Epoch 8 Loss 0.2437 Acc 0.8833\n",
      "Time taken for 1 epoch 2.2087249755859375 sec\n",
      "\n",
      "Epoch 9 Loss 0.2374 Acc 0.8855\n",
      "Time taken for 1 epoch 2.0335919857025146 sec\n",
      "\n",
      "Epoch 10 Loss 0.2314 Acc 0.8873\n",
      "Time taken for 1 epoch 2.0522079467773438 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    steps_per_epoch = 0\n",
    "\n",
    "    for x, y in train_dataset:\n",
    "        batch_loss, batch_acc = train_step(x, y)\n",
    "        total_loss += batch_loss\n",
    "        total_acc += batch_acc\n",
    "        steps_per_epoch += 1\n",
    "    \n",
    "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f} Acc {total_acc / steps_per_epoch:.4f}')\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_per_step(a, step=PIECE_LEN):\n",
    "    # add -1 to the end of each sample to make them the same length per step(piece_length)\n",
    "    pad_count = step - (a.shape[0] % step)\n",
    "    print(a.shape, end=' ')\n",
    "    if pad_count!=step : \n",
    "        print('pad by',pad_count, end=' ')\n",
    "        a = np.pad(a, ((0, pad_count), (0, 0)), 'constant', constant_values=-1)\n",
    "    # reshape into per step\n",
    "    a = np.reshape(a, (-1, step, a.shape[1]))\n",
    "    print('to',a.shape)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n"
     ]
    }
   ],
   "source": [
    "import midi_np_translation.output2midi as output2midi\n",
    "PATH = \"test_input\"\n",
    "# load np file\n",
    "test_file = np.load(\"preprocessed_dataset/irealpro_midi/Autumn Leaves_o0.mid.npy\")\n",
    "# test_file_truth = np.load(PATH + \"/\" + \"4on6.mid.ans.npy\")\n",
    "# output2midi.output_to_midi(bass_ndarr=test_file_truth, output_path=\"4on6_truth.mid\")\n",
    "test_result = model.predict(slice_per_step(test_file))\n",
    "# test_result = np.argmax(test_result, axis=2)\n",
    "output2midi.output_to_midi(bass_ndarr=test_result.reshape(-1,52), ref_midi_path=\"input_midi/irealpro_transposed/Autumn Leaves_o0.mid\", output_path=\"yo_al.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
